{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTOtL6oIn5tY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "from imageio import imread\n",
    "\n",
    "from skimage.transform import resize\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 49000\n",
    "NUM_VAL = 1000\n",
    "\n",
    "def process_raw_data():\n",
    "    t = time.time()\n",
    "    X = []\n",
    "    Y = []\n",
    "    train_folder = Path('../data/imgs/train')\n",
    "\n",
    "    for classname in train_folder.iterdir():\n",
    "        if classname.name != '.DS_Store':\n",
    "            for img in classname.iterdir():\n",
    "                X.append(imread(img))\n",
    "                Y.append(int(classname.name[1:]))\n",
    "    X = np.array(X).reshape(-1,3,480,640)\n",
    "    Y = np.array(Y)\n",
    "    Xtr, Xva, Ytr, Yva = model_selection.train_test_split(X, Y, test_size=0.25, random_state=20)\n",
    "    print('time used', time.time()-t)\n",
    "    return Xtr, Xva, Ytr, Yva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used 676.2898671627045\n",
      "Xtr (16818, 3, 480, 640)\n",
      "Ytr (16818,)\n",
      "Xva (5606, 3, 480, 640)\n",
      "Yva (5606,)\n"
     ]
    }
   ],
   "source": [
    "Xtr, Xva, Ytr, Yva = process_raw_data()\n",
    "print('Xtr', Xtr.shape)\n",
    "print('Ytr', Ytr.shape)\n",
    "print('Xva', Xva.shape)\n",
    "print('Yva', Yva.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_xtr = torch.from_numpy(Xtr)\n",
    "tensor_ytr = torch.from_numpy(Ytr)\n",
    "tensor_xva = torch.from_numpy(Xva)\n",
    "tensor_yva = torch.from_numpy(Yva)\n",
    "\n",
    "loader_tr = DataLoader(data.TensorDataset(tensor_xtr,tensor_ytr),batch_size=64,sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "loader_va = DataLoader(data.TensorDataset(tensor_xva,tensor_yva),batch_size=64,sampler=ChunkSampler(NUM_VAL, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 8, kernel_size=10, stride=2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.Conv2d(8, 16, kernel_size=10, stride=2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.MaxPool2d(2,stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(70224, 1024),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(1024, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(64, 3, 480, 640)\n",
    "x_var = Variable(x)\n",
    "ans = model(x_var)\n",
    "\n",
    "np.array(ans.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 100\n",
    "\n",
    "def train(model, loss_fn, optimizer, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_tr):\n",
    "            x_var = Variable(x.float())\n",
    "            y_var = Variable(y.long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "def check_accuracy(model):\n",
    "    print('Checking accuracy on validation set')  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    for x, y in loader_va:\n",
    "        x_var = Variable(x, volatile=True)\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 5\n",
      "t = 100, loss = 0.3668\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1e-1)\n",
    "train(model, loss_fn, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cs175_project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
